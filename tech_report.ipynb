{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634d5495",
   "metadata": {},
   "source": [
    "# Technical Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7bb38",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook walks through the creation of our ETL process for the video games sales database. For specific instructions on how to set up the database and perform the extract, transform, load, visit [README](https://github.com/asundquistdavis/Project-2/blob/main/README.md). We obtained data sets from [kaggle](https://www.kaggle.com/) as csv files. After that we were ready to start ETL.\n",
    "\n",
    "The first decision we made was to use a PostgreSQL database. Relational databases are typically easier to query. They can be harder to set up and limit what type of data can be added to the database. Since the data sets we are working with already have some relational structure, using SQL as opposed to MongoDB or another non-relational database is a natural choice. We created our database's ERD (shown below) using [quickDBD](https://www.quickdatabasediagrams.com/). The database is in 1st normal form - every element is atomic and cannot be broken down further. This facilitates easier data querying and manipulation. The database is not in 2nd normal form because the \"sales\" table lacks a single column primary key. Since this would not drastically impact the functionality, we decided against it to keep the model more simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89945339",
   "metadata": {},
   "source": [
    "![erd](project-2_database_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92edc15",
   "metadata": {},
   "source": [
    "## Extract\n",
    "The extraction step is straightforward; both data sets are downloaded as csv files. First we load them as dataframes using pandas csv reader and then check their contents for non-null data and make sure they make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# define the paths to the csv files that contain the raw data\n",
    "world_pop_path = \"Resources/world_population.csv\"\n",
    "vg_sales_path = \"Resources/vgsales.csv\"\n",
    "\n",
    "# read the population data as a dataframe  and preview it\n",
    "world_pop_df = pd.read_csv(world_pop_path)\n",
    "world_pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886911d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data types and amount of non-null data per column\n",
    "world_pop_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6376a9",
   "metadata": {},
   "source": [
    "The world population data contains 17 columns and all of them appear to contain 234 non-null values. In the transform step we will drop quite a bit of this data and clean it up so we are left with the data that is most useful for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77741ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the video games data as a dataframe and preview it\n",
    "vg_sales_df = pd.read_csv(vg_sales_path)\n",
    "vg_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view data types and amount of non-null data per column\n",
    "vg_sales_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7bd071",
   "metadata": {},
   "source": [
    "The video games sales data contains 11 columns with 16,598 non-null values in all the columns except publisher and release year. We could drop rows with incomplete data. However, for this project we are only concerned about sales data. So we keep all data as is and will drop unused columns in the transform process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f5dc9",
   "metadata": {},
   "source": [
    "## Transform\n",
    "First we work on creating the world population table. The table needs to store the population of each region - Japan, North America, Europe and the world. For this project, we only need population data for 2020. This is because our sales data are cumulative up until 2020.\n",
    "\n",
    "Japan's population is in the dataframe already so it can be loaded as is. To get the populations for each continent, we take the data and aggregate-sum it by continent. Finally, to get the world population, we sum over all the continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326423d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and sum all data by cntinent\n",
    "world_pop_by_continents = world_pop_df.groupby(world_pop_df['Continent']).sum()\n",
    "world_pop_by_continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract values for population table\n",
    "\n",
    "# na and eu populations are taken from the group by dataframe\n",
    "na_pop = world_pop_by_continents.loc['North America', '2020 Population']\n",
    "eu_pop = world_pop_by_continents.loc['Europe', '2020 Population']\n",
    "\n",
    "# wo population is calculated by summing over all continents\n",
    "wo_pop = world_pop_by_continents['2020 Population'].sum()\n",
    "\n",
    "# jp is seelected from the original df\n",
    "jp_pop = world_pop_df.loc[world_pop_df['Country']== 'Japan', '2020 Population'].values[0]\n",
    "\n",
    "#save region populations into table\n",
    "population = pd.DataFrame({\"region\":[\"na\", \"eu\", \"jp\", \"wo\"], \"population\":[na_pop, eu_pop, jp_pop, wo_pop]})\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d135361",
   "metadata": {},
   "source": [
    "This is the finished table, ready to be loaded into the database. Its primary key is region and the populations for each region are stored as whole numbers. \n",
    "\n",
    "Next we work on the video games table. Each row of the dataframe already represents a unique video game and platform pair. So we just need to assign a primary key to each row of the original dataframe and retain the video game and platform data. The column 'rank' from the original data frame indicates the game/platform pair's rank in total sales. While we are not interested in this number it can be used as the primary key as it is unique to each row of the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the rank cloumn as 'Game_Id'\n",
    "vg_sales_df= vg_sales_df.rename(columns={\"Rank\": \"game_id\", \"Name\": \"name\", \"Platform\": \"platfrom\"})\n",
    "\n",
    "# define the video games df \n",
    "video_games = vg_sales_df[[\"game_id\", \"name\", \"platform\"]]\n",
    "video_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706c12c",
   "metadata": {},
   "source": [
    "This is the video_games table ready to load into the database. \n",
    "\n",
    "Lastly we create the sales table. The original dataframe has sales by region in separate columns. We are interested in having unique elements for each game_Id **and** Region in the sales table. To obtain this we use Pandas to manipulate the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The melt function takes data from multpile columns and stores it as seperate elements\n",
    "sales=vg_sales_df.melt(value_vars=[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Global_Sales\"],id_vars=[\"Game_Id\"], \n",
    "                       var_name=\"region\", value_name=\"sales\")\n",
    "\n",
    "# rename the regions to match the region names\n",
    "sales[\"Region\"].replace({\"NA_Sales\": \"na\", \"EU_Sales\": \"eu\", \"JP_Sales\": \"jp\", \"Global_Sales\": \"wo\"}, inplace=True)\n",
    "\n",
    "sales[\"sales\"]= sales[\"sales\"]*1000000\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d545e0a",
   "metadata": {},
   "source": [
    "The resulting datafarame contains a column for Game_Id, Region and Sales and is ready to load into the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56532af2",
   "metadata": {},
   "source": [
    "## Load\n",
    "The last step is to load the data into our PostgreSQL database. We use pgAdmin to create and run the table [schema](schema.sql) for our database name \"project-2_db\". SQLalchemy creates the engine to interact with the PostgreSQL database and Pandas loads the dataframes as tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218533f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import methods sqlalchemy\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# configure PG admin and set up connection string\n",
    "protocol = 'postgresql'\n",
    "username = 'postgres'\n",
    "password = 'bootcamp'\n",
    "host = 'localhost'\n",
    "port = 5432\n",
    "database_name = 'project-2_db'\n",
    "rds_connection_string = f'{protocol}://{username}:{password}@{host}:{port}/{database_name}'\n",
    "\n",
    "# create Postgres engine\n",
    "engine = create_engine(rds_connection_string)\n",
    "\n",
    "# check to see if SQLalchemy found tables from the database\n",
    "inspector = inspect(engine)\n",
    "inspector.get_table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3263a",
   "metadata": {},
   "source": [
    "All three tables are found in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5516daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load population table into database\n",
    "population.to_sql(name='population', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# load video_games table into database\n",
    "video_games.to_sql(name='video_games', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# load sales table into database\n",
    "sales.to_sql(name='sales', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64446eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the video game table as a check that db is loaded!\n",
    "pd.read_sql_query('select * from video_games', con=engine).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd18c1",
   "metadata": {},
   "source": [
    "And now we have the data successfully loaded as a database ready to be analyzed! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
